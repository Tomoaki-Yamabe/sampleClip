{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/OlyKoek/MyStudy-AI-app/blob/feature-update-ml/create_multimodal_embeding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLj01Un8xV5g"
   },
   "source": [
    "# マルチモーダルなエンベディングモデルの作成\n",
    "# Mini-CLIP: Text + Image shared embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUXMBWYCsVfa"
   },
   "source": [
    "- Text: multilingual MiniLM (TinyBERT)\n",
    "- Image: MobileNetV3-Small\n",
    "- Projection Head: 256 dimention shared space\n",
    "- Loss: CLIP-style contrastive loss\n",
    "- VectorStore: SimpleVetorDB    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L44V18kXsVfa"
   },
   "source": [
    "# Install & Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KdIYRG3ysVfa",
    "outputId": "6581eed1-ee7d-4e78-a9f0-1051af85ae77"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --quiet\n",
    "!pip install transformers umap-learn matplotlib pandas scikit-learn plotly --quiet\n",
    "!pip install transformers --quiet\n",
    "\n",
    "!pip install kaggle\n",
    "\n",
    "import os\n",
    "import json\n",
    "from io import BytesIO\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "\n",
    "from PIL import Image\n",
    "import requests\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-BMU63pY1zt"
   },
   "source": [
    "# Config & Globl Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YB0aCKCMY7ib",
    "outputId": "b1ec673f-ddaf-46e5-dac0-5d2c49fe60c4"
   },
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "# Text Encorder Model\n",
    "TEXT_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "EMBED_DIM = 256\n",
    "\n",
    "# Decorder Model\n",
    "DECODER_NAME = \"google/flan-t5-small\"\n",
    "\n",
    "# Kaggle\n",
    "from google.colab import userdata\n",
    "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('username')\n",
    "os.environ[\"KAGGLE_KEY\"] = userdata.get('token')\n",
    "!kaggle datasets list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0HtGf0HEXEa5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FKq9_FRsVfb"
   },
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dhd5nuW4sVfb"
   },
   "outputs": [],
   "source": [
    "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    ベクトル a, b のコサイン類似度を返すための関数\n",
    "    \"\"\"\n",
    "    a_flat = a.flatten()\n",
    "    b_flat = b.flatten()\n",
    "    denom = (np.linalg.norm(a_flat) * np.linalg.norm(b_flat) + 1e-8)\n",
    "    return float(np.dot(a_flat, b_flat) / denom)\n",
    "\n",
    "\n",
    "def load_image(x):\n",
    "    \"\"\"\n",
    "    Imageを読み込むための関数\n",
    "    入力 x:\n",
    "      - str(URL or file path)\n",
    "      - PIL.Image.Image\n",
    "    \"\"\"\n",
    "    if isinstance(x, Image.Image):\n",
    "        return x.convert(\"RGB\")\n",
    "\n",
    "    if isinstance(x, str):\n",
    "        if x.startswith(\"https://\") or x.startswith(\"http://\"):\n",
    "            response = requests.get(x)\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "            return img.convert(\"RGB\")\n",
    "        else:\n",
    "            img = Image.open(x)\n",
    "            return img.convert(\"RGB\")\n",
    "\n",
    "    raise ValueError(f\"Unsupported image input type: {type(x)}\")\n",
    "\n",
    "\n",
    "def plot_umap_matplotlib(embs, labels, texts=None, title=\"UMAP\"):\n",
    "    \"\"\"\n",
    "    MatplotlibでUMAPを描画\n",
    "    embs: (N, D) numpy array\n",
    "    labels: list[str] same length as N\n",
    "    texts:  list[str] hover表示用（任意）\n",
    "    \"\"\"\n",
    "    reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric=\"cosine\")\n",
    "    coords = reducer.fit_transform(embs)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for i, lab in enumerate(labels):\n",
    "        plt.scatter(coords[i, 0], coords[i, 1])\n",
    "        if texts is not None:\n",
    "            plt.text(coords[i, 0], coords[i, 1], texts[i][:10], fontsize=8)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_umap_plotly(embs, labels, texts=None, extra_meta=None, title=\"UMAP (Interactive)\"):\n",
    "    \"\"\"\n",
    "    PlotlyでインタラクティブなUMAPを描画\n",
    "    \"\"\"\n",
    "    reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric=\"cosine\")\n",
    "    coords = reducer.fit_transform(embs)\n",
    "\n",
    "    data = {\n",
    "        \"x\": coords[:, 0],\n",
    "        \"y\": coords[:, 1],\n",
    "        \"type\": labels,\n",
    "    }\n",
    "    if texts is not None:\n",
    "        data[\"text\"] = texts\n",
    "    if extra_meta is not None:\n",
    "        for k, v in extra_meta.items():\n",
    "            data[k] = v\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        color=\"type\",\n",
    "        hover_data=list(data.keys()),\n",
    "        title=title,\n",
    "        width=800,\n",
    "        height=600,\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "def plot_umap_plotly_with_images(embs, labels, texts, image_paths, title=\"UMAP with Images\"):\n",
    "    \"\"\"\n",
    "    PlotlyでインタラクティブなUMAPを描画し、画像も表示\n",
    "    \"\"\"\n",
    "    reducer = umap.UMAP(n_neighbors=10, min_dist=0.1, metric=\"cosine\")\n",
    "    coords = reducer.fit_transform(embs)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"x\": coords[:, 0],\n",
    "        \"y\": coords[:, 1],\n",
    "        \"label\": labels,\n",
    "        \"text\": texts,\n",
    "        \"image_path\": image_paths,\n",
    "    })\n",
    "\n",
    "    # hoverに画像を埋め込む（HTML <img> タグを使用）\n",
    "    df[\"customdata\"] = df[\"image_path\"]\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"x\",\n",
    "        y=\"y\",\n",
    "        color=\"label\",\n",
    "        hover_name=\"text\",\n",
    "        hover_data={\"image_path\": False},\n",
    "        custom_data=[\"customdata\"],\n",
    "        width=900,\n",
    "        height=650,\n",
    "        title=title,\n",
    "    )\n",
    "\n",
    "    # hovertemplate を書き換える\n",
    "    fig.update_traces(\n",
    "        hovertemplate=\n",
    "        \"<b>%{hovertext}</b><br><br>\" +\n",
    "        \"<img src='%{customdata}' style='width:120px;height:auto;'><br>\" +\n",
    "        \"<extra></extra>\"\n",
    "    )\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "def unpickle(file):\n",
    "    import pickle\n",
    "    with open(file, 'rb') as fo:\n",
    "        return pickle.load(fo, encoding='bytes')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1Q39TFCsVfb"
   },
   "source": [
    "## Core Classes\n",
    "- TextEncoder\n",
    "- ImageEncoder\n",
    "- MiniCLIP\n",
    "- SimpleVectorDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJ5AezfqsVfc"
   },
   "outputs": [],
   "source": [
    "class TextEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    TinyBERT系でテキストをエンコードするクラス\n",
    "    Linear Projectionで256次元に射影変換\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str, out_dim: int = EMBED_DIM, device: torch.device = DEVICE):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name).to(device)\n",
    "        hidden = self.model.config.hidden_size\n",
    "        self.projector  = nn.Linear(hidden, out_dim).to(device)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        HuggingFaceスタイルのForward関数を参考に実装\n",
    "        B: バッチサイズ\n",
    "        3: チャネル数(RGB)\n",
    "        H, W: 画像の高さと幅\n",
    "        T: トークン数\n",
    "        H: モデルの隠れ層次元数\n",
    "        out_dim: 射影後の次元数\n",
    "        \"\"\"\n",
    "        outputs = self.model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)  # (B, T, H) -> (B, H)\n",
    "        projected = self.projector(embeddings)  # (B, out_dim)\n",
    "        return projected\n",
    "\n",
    "    def encode(self, texts, normalize: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        textsをエンコードしてベクトルを返す関数\n",
    "\n",
    "        texts: list[str]\n",
    "        normalize: bool - 出力ベクトルを正規化するかどうか\n",
    "        \"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "\n",
    "        inputs = self.tokenizer(\n",
    "            texts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            emb = outputs.last_hidden_state.mean(dim=1)  # (B, H)\n",
    "\n",
    "        proj = self.projector(emb)  # (B, out_dim)\n",
    "\n",
    "        if normalize:\n",
    "            proj = proj / (proj.norm(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "        return proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oCmyFnLHsVfc"
   },
   "outputs": [],
   "source": [
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    MobileNetV3-Smallで画像をエンコードするクラス\n",
    "    Linear Projectionで256次元に射影変換\n",
    "    \"\"\"\n",
    "    def __init__(self, out_dim: int = EMBED_DIM, device: torch.device = DEVICE):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.model = models.mobilenet_v3_small(pretrained=True)\n",
    "        self.model.eval()\n",
    "        self.features = self.model.features.to(device)\n",
    "        self.projector = nn.Linear(576, out_dim).to(device)  # MobileNetV3-Smallの最終特徴量次元数は576\n",
    "\n",
    "        self.transform = T.Compose([\n",
    "            T.Resize((224, 224)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485, 0.456, 0.406],\n",
    "                        [0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "    def encode(self, images, normalize: bool = True) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        画像リストをエンコードしてベクトルを返す関数\n",
    "\n",
    "        images: list[PIL.Image.Image] または 単一のPIL.Image.Image\n",
    "        normalize: bool - 出力ベクトルを正規化するかどうか\n",
    "        \"\"\"\n",
    "        single = False\n",
    "        if not isinstance(images, (list, tuple)):\n",
    "            images = [images]\n",
    "            single = True\n",
    "\n",
    "        tensors = []\n",
    "        for img in images:\n",
    "            pil_img = load_image(img)\n",
    "            tensor = self.transform(pil_img) # (3, H, W)\n",
    "            tensors.append(tensor)\n",
    "        batch = torch.stack(tensors, dim=0).to(self.device)  # (B, 3, H, W)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat = self.features(batch)  # (B, 576, 7, 7)\n",
    "            feat = feat.mean(dim=[2, 3])  # (B, 576)\n",
    "\n",
    "        proj = self.projector(feat)  # (B, out_dim)\n",
    "\n",
    "        if normalize:\n",
    "            proj = proj / (proj.norm(dim=1, keepdim=True) + 1e-8)\n",
    "\n",
    "        if single:\n",
    "            return proj # (1, out_dim)\n",
    "        return proj  # (B, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKkPeBRlsVfc"
   },
   "outputs": [],
   "source": [
    "class MiniCLIP:\n",
    "    \"\"\"\n",
    "    TextEncoder と ImageEncoder を共有 256次元空間に揃え、\n",
    "    CLIP風の対照学習で projector 層のみを学習するクラス。\n",
    "    \"\"\"\n",
    "    def __init__(self, text_encoder: TextEncoder, image_encoder: ImageEncoder, temperature: float = 0.07):\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_encoder = image_encoder\n",
    "        self.temperature = temperature\n",
    "\n",
    "        params = list(self.text_encoder.projector.parameters()) + \\\n",
    "                 list(self.image_encoder.projector.parameters())\n",
    "        self.optimizer = AdamW(params, lr=1e-4)\n",
    "\n",
    "\n",
    "    def compute_loss(self, img_vecs: torch.Tensor, txt_vecs: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        CLIP風の対照学習の損失を計算する関数\n",
    "        img_vecs: (B, D)\n",
    "        txt_vecs: (B, D)\n",
    "        \"\"\"\n",
    "        # 類似度行列[B, B]を計算\n",
    "        sim_matrix = torch.matmul(img_vecs, txt_vecs.T)\n",
    "        sim_matrix = sim_matrix / self.temperature\n",
    "\n",
    "        labels = torch.arange(len(img_vecs), device=sim_matrix.device)\n",
    "\n",
    "        loss_img2txt = F.cross_entropy(sim_matrix, labels)\n",
    "        loss_txt2img = F.cross_entropy(sim_matrix.T, labels)\n",
    "\n",
    "        loss = (loss_img2txt + loss_txt2img) / 2.0\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def train(self, pairs, epochs=10, batch_size=32):\n",
    "        \"\"\"\n",
    "        学習を実行する関数\n",
    "        pairs: list of (PIL.Image.Image, str)\n",
    "        \"\"\"\n",
    "        n = len(pairs)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            perm = np.random.permutation(n)\n",
    "            epoch_loss = 0.0\n",
    "\n",
    "            for start in range(0, n, batch_size):\n",
    "                end = start + batch_size\n",
    "                idx = perm[start:end]\n",
    "\n",
    "                batch = [pairs[i] for i in idx]\n",
    "                texts  = [p[\"text\"]  for p in batch]\n",
    "                images = [p[\"image\"] for p in batch]\n",
    "\n",
    "                # normalize=False → CLIP の標準\n",
    "                txt_vecs = self.text_encoder.encode(texts, normalize=False)\n",
    "                img_vecs = self.image_encoder.encode(images, normalize=False)\n",
    "\n",
    "                loss = self.compute_loss(img_vecs, txt_vecs)\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                epoch_loss += loss.item() * len(batch)\n",
    "\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/n:.4f}\")\n",
    "\n",
    "        print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xEMLg3XFsqnx"
   },
   "outputs": [],
   "source": [
    "# # -----------------------------\n",
    "# # Prepare tokenizer & training data for text decoder\n",
    "# # -----------------------------\n",
    "\n",
    "# from transformers import AutoTokenizer\n",
    "# tokenizer_lm = AutoTokenizer.from_pretrained(TEXT_MODEL_NAME)\n",
    "\n",
    "# # 学習用：MemeCap のテキストだけ集める\n",
    "# train_texts = [p[\"text\"] for p in pairs]\n",
    "\n",
    "# def encode_text_for_decoder(text, max_len=20):\n",
    "#     t = tokenizer_lm(\n",
    "#         text,\n",
    "#         padding=\"max_length\",\n",
    "#         truncation=True,\n",
    "#         max_length=max_len,\n",
    "#         return_tensors=\"pt\"\n",
    "#     )\n",
    "#     ids = t[\"input_ids\"][0]  # (max_len)\n",
    "#     return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-f7cI72LsYo3"
   },
   "outputs": [],
   "source": [
    "class TransformerTextDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim, vocab_size, hidden_dim=256, num_layers=3, max_len=40, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # --- 1) Token embedding ---\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # --- 2) Positional encoding ---\n",
    "        self.pos_embed = nn.Embedding(max_len, embed_dim)\n",
    "\n",
    "        # --- 3) Transformer decoder ---\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=8,\n",
    "            dim_feedforward=hidden_dim,\n",
    "            dropout=0.1,\n",
    "            batch_first=False  # (T,B,D) 形式で処理する\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # --- 4) vocab projection ---\n",
    "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, memory, tgt):\n",
    "        \"\"\"\n",
    "        memory: (1,1,256)\n",
    "        tgt:    (1, T)\n",
    "        \"\"\"\n",
    "        B, T = tgt.shape\n",
    "\n",
    "        # ---- 1) token embedding ----\n",
    "        tgt_emb = self.embed(tgt)         # (1,T,256)\n",
    "\n",
    "        # ---- 2) add position ----\n",
    "        pos_ids = torch.arange(T, device=self.device).unsqueeze(0)  # (1,T)\n",
    "        pos_emb = self.pos_embed(pos_ids)  # (1,T,256)\n",
    "\n",
    "        tgt_emb = tgt_emb + pos_emb       # (1,T,256)\n",
    "\n",
    "        # ---- 3) Transformer expects (T,B,256) ----\n",
    "        tgt_emb = tgt_emb.transpose(0, 1)  # (T,1,256)\n",
    "\n",
    "        # ---- 4) decode ----\n",
    "        out = self.decoder(tgt=tgt_emb, memory=memory)   # (T,1,256)\n",
    "\n",
    "        # ---- 5) vocab projection ----\n",
    "        out = self.fc(out)  # (T,1,vocab)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HrchQKCWsVfd"
   },
   "outputs": [],
   "source": [
    "class SimpleVectorDB:\n",
    "    \"\"\"\n",
    "    非常にシンプルなベクトルストア。\n",
    "    - items: List[{\"vec\": np.ndarray, \"type\": ..., \"text\": ..., \"image\": ...}]\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.items = []\n",
    "\n",
    "    def add(self, vec: np.ndarray, metadata: dict):\n",
    "        self.items.append({\n",
    "            \"vec\": vec.astype(\"float32\"),\n",
    "            **metadata\n",
    "        })\n",
    "\n",
    "    def build_from_pairs(self, text_encoder, image_encoder, pairs):\n",
    "        self.items = []\n",
    "        for p in pairs:\n",
    "            text = p[\"text\"]\n",
    "            img = p[\"image\"]\n",
    "            img_path = p.get(\"image_path\", None)\n",
    "\n",
    "            # text embedding\n",
    "            t_vec = text_encoder.encode(text).cpu().detach().numpy()[0]\n",
    "            self.add(t_vec, {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": text,\n",
    "                \"image_path\": img_path,\n",
    "            })\n",
    "\n",
    "            # image embedding\n",
    "            i_vec = image_encoder.encode(img).cpu().detach().numpy()[0]\n",
    "            self.add(i_vec, {\n",
    "                \"type\": \"image\",\n",
    "                \"text\": text,\n",
    "                \"image_path\": img_path,\n",
    "            })\n",
    "\n",
    "\n",
    "    def search(self, query_vec: np.ndarray, top_k: int = 5, type_filter: str = None):\n",
    "        \"\"\"\n",
    "        query_vec: np.ndarray [D]\n",
    "        type_filter: \"text\" or \"image\" or None\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        for item in self.items:\n",
    "            if type_filter is not None and item[\"type\"] != type_filter:\n",
    "                continue\n",
    "            sim = cosine_sim(query_vec, item[\"vec\"])\n",
    "            results.append((sim, item))\n",
    "\n",
    "        results.sort(key=lambda x: x[0], reverse=True)\n",
    "        return results[:top_k]\n",
    "\n",
    "    def to_jsonable(self):\n",
    "          json_items = []\n",
    "          for item in self.items:\n",
    "              j = dict(item)\n",
    "              j.pop(\"image\", None)\n",
    "              j[\"vec\"] = item[\"vec\"].tolist()\n",
    "              json_items.append(j)\n",
    "          return json_items\n",
    "\n",
    "    @staticmethod\n",
    "    def from_json(data):\n",
    "        db = SimpleVectorDB()\n",
    "        for item in data:\n",
    "            vec = np.array(item[\"vec\"], dtype=\"float32\")\n",
    "            meta = {k: v for k, v in item.items() if k != \"vec\"}\n",
    "            db.add(vec, meta)\n",
    "        return db\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z62uxGG1aojb"
   },
   "source": [
    "# Decorder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yTUE9gxxKvIP"
   },
   "outputs": [],
   "source": [
    "tiny_tok = AutoTokenizer.from_pretrained(DECODER_NAME)\n",
    "tiny_model = AutoModelForSeq2SeqLM.from_pretrained(DECODER_NAME).to(DEVICE)\n",
    "\n",
    "def generate_tiny_summary(user_query, related_texts, lang=\"en\", max_new_tokens=96):\n",
    "    context = \" | \".join(related_texts)\n",
    "\n",
    "    if lang == \"jp\":\n",
    "        prompt = (\n",
    "            \"以下の内容を踏まえて、ユーザーの気持ちや意図を日本語で短く説明してください。\\n\"\n",
    "            \"・文章をコピーせず、新しい文章を生成すること\\n\"\n",
    "            \"・関連キャプションは参考情報であり、要約ではない\\n\"\n",
    "            f\"ユーザー入力: {user_query}\\n\"\n",
    "            f\"関連キャプション: {context}\\n\"\n",
    "            \"説明（日本語）:\"\n",
    "        )\n",
    "    else:\n",
    "        prompt = (\n",
    "            \"Write a short, original English explanation about the user's feeling.\\n\"\n",
    "            \"Do NOT copy any sentences. Generate a new interpretation.\\n\"\n",
    "            f\"User Input: {user_query}\\n\"\n",
    "            f\"Related Meme Captions: {context}\\n\"\n",
    "            \"Explanation:\"\n",
    "        )\n",
    "\n",
    "    inputs = tiny_tok(prompt, return_tensors=\"pt\", truncation=True).to(DEVICE)\n",
    "\n",
    "    outputs = tiny_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=max_new_tokens,\n",
    "        do_sample=True,\n",
    "        top_p=0.92,\n",
    "        temperature=0.85,\n",
    "    )\n",
    "    text = tiny_tok.decode(outputs[0], skip_special_tokens=True)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "\n",
    "def explain_with_decoder(query_text=None, query_image=None, top_k=3, lang=\"jp\"):\n",
    "    assert query_text or query_image\n",
    "\n",
    "    # 1) Encode\n",
    "    if query_text:\n",
    "        query_vec = text_encoder.encode(query_text).detach().cpu().numpy()[0]\n",
    "    else:\n",
    "        img = load_image(query_image)\n",
    "        query_vec = image_encoder.encode(img).detach().cpu().numpy()[0]\n",
    "\n",
    "    # 2) VectorDB 検索\n",
    "    results = vecdb.search(query_vec, top_k=top_k)\n",
    "\n",
    "    # 3) 関連テキスト抽出\n",
    "    related_texts = [item[\"text\"] for _, item in results]\n",
    "\n",
    "    # 4) T5で生成\n",
    "    generated = generate_tiny_summary(\n",
    "        user_query=query_text or \"(image query)\",\n",
    "        related_texts=related_texts,\n",
    "        lang=lang,\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"query_text\": query_text,\n",
    "        \"related_texts\": related_texts,\n",
    "        \"generated\": generated,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prATT5pC0OBx"
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N4fNekJw0Q5M"
   },
   "outputs": [],
   "source": [
    "# # @title\n",
    "# class ArtBenchDatasetLoader:\n",
    "#     \"\"\"\n",
    "#     ArtBench-10 (CIFAR形式) ローダー\n",
    "#     - b\"data\" / \"data\" どちらにも対応\n",
    "#     - Kaggle版 / GitHub版 どちらも対応\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, root_dir, limit_per_class=200, shuffle=True):\n",
    "#         self.root = root_dir\n",
    "#         self.limit = limit_per_class\n",
    "#         self.shuffle = shuffle\n",
    "\n",
    "#         # Python版のデータディレクトリ\n",
    "#         self.py_dir = os.path.join(root_dir, \"artbench-10-python\", \"artbench-10-batches-py\")\n",
    "\n",
    "#         # クラス名は meta ファイルから読み込み\n",
    "#         meta_path = os.path.join(self.py_dir, \"meta\")\n",
    "#         meta = unpickle(meta_path)\n",
    "\n",
    "#         # meta は {b\"styles\": [...]} or {\"styles\": [...]} のどちらか\n",
    "#         if b\"styles\" in meta:\n",
    "#             self.styles = [s.decode(\"utf-8\") for s in meta[b\"styles\"]]\n",
    "#         else:\n",
    "#             self.styles = meta[\"styles\"]\n",
    "\n",
    "#         print(\"Loaded styles:\", self.styles)\n",
    "\n",
    "#     def _get(self, batch, key):\n",
    "#         \"\"\"\n",
    "#         batch[b\"data\"] と batch[\"data\"] 両方に対応するヘルパー\n",
    "#         \"\"\"\n",
    "#         if key in batch:\n",
    "#             return batch[key]\n",
    "#         if key.encode() in batch:\n",
    "#             return batch[key.encode()]\n",
    "#         raise KeyError(f\"key {key} not found in batch\")\n",
    "\n",
    "#     def _load_batch(self, file_path):\n",
    "#         batch = unpickle(file_path)\n",
    "\n",
    "#         data = self._get(batch, \"data\").reshape(-1, 3, 32, 32)\n",
    "#         labels = self._get(batch, \"labels\")\n",
    "#         return data, labels\n",
    "\n",
    "#     def _to_pil(self, arr):\n",
    "#         img = np.transpose(arr, (1,2,0))\n",
    "#         return Image.fromarray(img.astype(np.uint8))\n",
    "\n",
    "#     def load(self):\n",
    "#         batch_files = sorted([\n",
    "#             os.path.join(self.py_dir, f)\n",
    "#             for f in os.listdir(self.py_dir)\n",
    "#             if f.startswith(\"data_batch\") or f==\"test_batch\"\n",
    "#         ])\n",
    "\n",
    "#         pairs = []\n",
    "#         per_class = {i:0 for i in range(10)}\n",
    "\n",
    "#         for bf in batch_files:\n",
    "#             data, labels = self._load_batch(bf)\n",
    "\n",
    "#             for i, lid in enumerate(labels):\n",
    "#                 if per_class[lid] >= self.limit:\n",
    "#                     continue\n",
    "\n",
    "#                 img = self._to_pil(data[i])\n",
    "#                 text = f\"{self.styles[lid]} の絵画\"\n",
    "\n",
    "#                 pairs.append({\n",
    "#                     \"image\": img,\n",
    "#                     \"text\": text,\n",
    "#                 })\n",
    "\n",
    "#                 per_class[lid] += 1\n",
    "\n",
    "#         if self.shuffle:\n",
    "#             np.random.shuffle(pairs)\n",
    "\n",
    "#         print(\"Loaded\", len(pairs), \"image-text pairs\")\n",
    "#         return pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WkywZJ2L2mfK"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "class MemeCapLoader:\n",
    "    def __init__(self, root_dir, split=\"train\", limit=None, shuffle=True, text_mode=\"meme\"):\n",
    "        self.root = root_dir\n",
    "        self.split = split\n",
    "        self.limit = limit\n",
    "        self.shuffle = shuffle\n",
    "        self.text_mode = text_mode\n",
    "\n",
    "        json_name = \"memes-trainval.json\" if split == \"train\" else \"memes-test.json\"\n",
    "        json_path = os.path.join(root_dir, json_name)\n",
    "\n",
    "        with open(json_path, \"r\") as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        print(f\"[MemeCap] loaded {len(self.data)} items\")\n",
    "\n",
    "        # 画像ファイルを全探索して index を作る\n",
    "        self.image_index = self._build_image_index()\n",
    "\n",
    "    def _build_image_index(self):\n",
    "        print(\"[MemeCap] scanning image files...\")\n",
    "        index = {}\n",
    "        for dirpath, _, filenames in os.walk(self.root):\n",
    "            for fname in filenames:\n",
    "                if fname.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                    index[fname] = os.path.join(dirpath, fname)\n",
    "        print(f\"[MemeCap] found {len(index)} image files\")\n",
    "        return index\n",
    "\n",
    "    def _select_text(self, item):\n",
    "        if self.text_mode == \"meme\":\n",
    "            return item[\"meme_captions\"][0]\n",
    "        elif self.text_mode == \"image\":\n",
    "            return item[\"img_captions\"][0]\n",
    "        elif self.text_mode == \"title\":\n",
    "            return item[\"title\"]\n",
    "        elif self.text_mode == \"mixed\":\n",
    "            return f\"{item['title']} / {item['img_captions'][0]} / {item['meme_captions'][0]}\"\n",
    "        else:\n",
    "            return item[\"meme_captions\"][0]\n",
    "\n",
    "    def load(self):\n",
    "        pairs = []\n",
    "        data_iter = self.data if self.limit is None else self.data[:self.limit]\n",
    "\n",
    "        for item in data_iter:\n",
    "            fname = item[\"img_fname\"]\n",
    "            img_path = self.image_index.get(fname)\n",
    "            if not img_path:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "            text = self._select_text(item)\n",
    "            pairs.append({\"image\": img, \"image_path\": img_path, \"text\": text})\n",
    "\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(pairs)\n",
    "\n",
    "        print(f\"[MemeCap] Loaded {len(pairs)} pairs\")\n",
    "        return pairs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I6JtaxwPsVfd"
   },
   "source": [
    "# MiniClip作って学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CRzscPEFFw2P",
    "outputId": "8c2efd5d-154e-4cd6-b261-759244a95843"
   },
   "outputs": [],
   "source": [
    "# DownloadDataset\n",
    "# !mkdir -p ./artbench\n",
    "# !kaggle datasets download alexanderliao/artbench10 -p ./artbench --unzip\n",
    "\n",
    "!mkdir -p ./meme\n",
    "!kaggle datasets download harshittiwari007/meme-convx -p ./meme --unzip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YJxCsh5HsVfd",
    "outputId": "1047d2a9-571b-4f08-ddd8-c951736a633e"
   },
   "outputs": [],
   "source": [
    "loader = MemeCapLoader(\n",
    "    \"./meme\",\n",
    "    split=\"train\",\n",
    "    limit=1000,        # とりあえず軽く\n",
    "    text_mode=\"meme\",  # まずはミームの意味キャプションで\n",
    ")\n",
    "\n",
    "pairs = loader.load()\n",
    "print(\"pairs:\", len(pairs))\n",
    "display(pairs[0][\"image\"])\n",
    "print(pairs[0][\"image_path\"])\n",
    "print(pairs[0][\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 917,
     "referenced_widgets": [
      "9918b290326a45179c223625f22a78f1",
      "f8288642a4884996b4f9a6b010768b8b",
      "349ea6930c514f07b4f8d8b564834c26",
      "e0b93b9f08324ad9972f855286bfaed6",
      "efb50617c9fa4647ab2036a044e56b84",
      "505caea7fe6d488a937f17c3f1a7c2d9",
      "7d94286856c7414ba949142302a96bbd",
      "1c4ef91a33cb41bab9dd7d6b613da9eb",
      "46048f991d8d49fd902f6e17979bdde1",
      "51ef320eb3594fc0b8c2349376fde833",
      "0574be944dc4410c998d10713cf96330",
      "2a67b2d71cf046e79ba40dec57743860",
      "3d63c4d0e45547c7b08407d714f6e598",
      "1153ec82de404c52a3e69f26327b1ec2",
      "4c17d312dd1b4c28ae60335153c6f48f",
      "4cadc54a83b042af9013ee61facb9d7b",
      "0ce7d7cf88dd4c90ad6523ef037f9600",
      "a9b60eff2c54482d92b99fd2e1a48e36",
      "e41c0d7fd0b94a1f85aa40152a6bcba5",
      "b85db7e2d665483287d04c68861caa5b",
      "c1d6c28dde754d1ab980990b05f53419",
      "42428fdbfe6240d18c121215646dd139",
      "e234122074fe49159c2c3ea059d75792",
      "297e652a4c04441c8d0deacc9bd291f3",
      "70b05495d0944cd5b2e9adf83dc00944",
      "7310d9419b9b47b49e2deaa47ac5c798",
      "5b90860ee5d34709b9b3b82282af3aae",
      "9f9da75e23b44bc7b123723e19625754",
      "a05c58f5d92f44b48ed3aefad6ff7ae7",
      "1dbe1913b3d347be911c036d96484a3a",
      "f7b7958aec1f4cbaafd7844b33750581",
      "bfedc9533bf047f3aebe6aed2a8d225f",
      "c15809057dbb4a67b5e49a203f2e5a99",
      "ca2d1336334a4ce48cf9c9a220d80471",
      "11b06887f54640c1948c0b507ee54d94",
      "58169d25ff1e41c197fb85cd1205080b",
      "9bd791534b92496d8b62710b6f5e1549",
      "00a2345382fc43d5922ae0c11d623322",
      "5cc2002ce45040f3af7ea7e8d031349d",
      "53283e69a44f4a9a9a1e1ec57fb47074",
      "f897d04fe0754d99a2eb1c15108eb147",
      "bf2eef3aa0c040fcacbfcedc456d0ed1",
      "36cfc34ec46742b4ab7a3f09ecf05da0",
      "b3c2610ca764465ba8da0299e3ade63c",
      "85b490dba1584328ae0d6326280f0dc3",
      "825ea29ca84046ad94c0cc86400cea11",
      "ac88f77ed0014a64b686c06ddbbb1301",
      "b55e6472b87b48858c751f3a59b1bf53",
      "a32326fc92394c45bd139f2f171273f5",
      "28c3fa5d27c64f649f966d8faa59384a",
      "c3028edba5214947a0b45a1deb021bf8",
      "ffec7f8706a448df8b86f90396f4341e",
      "e569381e9c5e44599b1587ae220f52e5",
      "d3d8dd2863f4437ca1e747d7bf5b2d04",
      "7bd07d011274471089ee412e88106171"
     ]
    },
    "id": "PnNidz9TPaO2",
    "outputId": "c43016d2-19b9-4d6b-def4-d6c8d37a6254"
   },
   "outputs": [],
   "source": [
    "# MiniCLIPモデルを初期化\n",
    "text_encoder = TextEncoder(TEXT_MODEL_NAME, EMBED_DIM, DEVICE)\n",
    "image_encoder = ImageEncoder(EMBED_DIM, DEVICE)\n",
    "mini_clip = MiniCLIP(text_encoder, image_encoder)\n",
    "\n",
    "# 学習開始\n",
    "print(\"Mini-CLIP training start...\")\n",
    "mini_clip.train(pairs, epochs=30, batch_size=32)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99PuI5zzsVfd"
   },
   "source": [
    "# VectorDB構築＆検索＆可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PKDt7CYbsVfd",
    "outputId": "26efb990-7584-45d4-9821-db075db5284a"
   },
   "outputs": [],
   "source": [
    "# VectorDB構築\n",
    "vecdb = SimpleVectorDB()\n",
    "vecdb.build_from_pairs(text_encoder, image_encoder, pairs)\n",
    "print(f\"VectorDB size: {len(vecdb.items)} items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXKn90TgSRuy"
   },
   "source": [
    "# VectorDBテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 859
    },
    "id": "0GbIF-8mSUBe",
    "outputId": "282b00c0-1749-4cb6-8199-2ec2af6a8329"
   },
   "outputs": [],
   "source": [
    "# テキストでクエリ\n",
    "query_text = \"今日は本当にめんどい\"\n",
    "query_vec = text_encoder.encode(query_text).detach().cpu().numpy()[0]\n",
    "\n",
    "results = vecdb.search(query_vec, top_k=5)\n",
    "\n",
    "for sim, item in results:\n",
    "    print(f\"sim={sim:.3f} | type={item['type']} | text={item['text']}\")\n",
    "    # display(item[\"image_path\"].resize((150, 150)))\n",
    "    img = Image.open(item[\"image_path\"])\n",
    "    display(img.resize((150,150)))\n",
    "\n",
    "\n",
    "# imageでクエリ\n",
    "# img = image_text_pairs[0][\"image\"]\n",
    "# img_vec = image_encoder.encode(img).detach().cpu().numpy()[0]\n",
    "\n",
    "# results = vecdb.search(img_vec, top_k=5)\n",
    "\n",
    "# for sim, item in results:\n",
    "#     print(f\"sim={sim:.3f} | text={item['text']}\")\n",
    "#     display(item[\"image\"].resize((150,150)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8U_Yoi0yJums",
    "outputId": "f920fd38-e92d-4965-b939-89763e5a299a"
   },
   "outputs": [],
   "source": [
    "result = explain_with_decoder(query_text=\"たくさんの猫が欲しい\")\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "40bEOaRtsVfe"
   },
   "source": [
    "# UMAP 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 667
    },
    "id": "WA-Z8pMssVfe",
    "outputId": "68aaf352-804b-4214-8a4e-6cc4c5e36ce7"
   },
   "outputs": [],
   "source": [
    "embs = np.stack([it[\"vec\"] for it in vecdb.items], axis=0)\n",
    "labels = [it[\"type\"] for it in vecdb.items]\n",
    "texts = [it[\"text\"] for it in vecdb.items]\n",
    "image_paths = [it[\"image_path\"] for it in vecdb.items]\n",
    "\n",
    "plot_umap_plotly_with_images(embs, labels, texts, image_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hBAR4rwsVfe"
   },
   "source": [
    "# Export image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w8YgHYhfsVfe",
    "outputId": "2a1deafa-1eba-44c1-bae0-945051c24502"
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "\n",
    "# projector 保存\n",
    "torch.save(text_encoder.projector.state_dict(), \"text_projector.pt\")\n",
    "torch.save(image_encoder.projector.state_dict(), \"image_projector.pt\")\n",
    "\n",
    "# VectorDB 保存\n",
    "with open(\"vector_db.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(vecdb.to_jsonable(), f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AARYzRXm1AT8"
   },
   "source": [
    "#### 日本語表示対応"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjxD2GP7sVfe"
   },
   "outputs": [],
   "source": [
    "# # 日本語フォントをダウンロードする。\n",
    "# !apt-get -y install fonts-ipafont-gothic\n",
    "# import matplotlib.pyplot as plt\n",
    "# plt.rcParams['font.family'] = 'IPAGothic'\n",
    "\n",
    "# import shutil\n",
    "# import os\n",
    "# # フォントキャッシュを削除\n",
    "# font_cache_path = os.path.expanduser(\"~/.cache/matplotlib\")\n",
    "# if os.path.exists(font_cache_path):\n",
    "#     shutil.rmtree(font_cache_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPlWea1rr5Q9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
